{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3DHsI4K3up6"
      },
      "source": [
        "### EEGNet() w/PyTorch\n",
        "# Utsav Dutta, Medical Intelligence and Language Engineering Lab\n",
        "# IISc Bangalore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9g4D8CqCfwY"
      },
      "source": [
        "# Import relevant PyTorch libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nefnnEyPe-C4"
      },
      "source": [
        "## Define a Dataset class module\n",
        "\n",
        "'''\n",
        "  - Root: Define root folder containing data in the form of the following:\n",
        "  /Class1\n",
        "    -S1\n",
        "    -S2\n",
        "    -S3..\n",
        "  /Class2\n",
        "    -S1\n",
        "    -S2\n",
        "    -S3..\n",
        "  where each class contains the same subjects.\n",
        "\n",
        "  - Index Map: \n",
        "    Mapping function from integer (0 to N), to [class,subject] mapping. \n",
        "    The helper function 'return_index' returns this mapping as a dictionary\n",
        "\n",
        "  - __getitem__:\n",
        "    Lazy load file when called for. Maps index to [class,subject] to build path.\n",
        "    Epoching of data is done WITHIN getitem and returns a tuple of sizes:\n",
        "    X -> [N_epochs,Channels,Times], Y -> [N_epochs, Labels]\n",
        "    \n",
        "    We define a custom_collate function to concatenate getitem calls instead of\n",
        "    stacking on first dimension as done by PyTorch's default mechanism.\n",
        "\n",
        "'''\n",
        "class EEGDataset(Dataset):\n",
        "\n",
        "  # Define self variables\n",
        "  def __init__(self,root = '/content/drive/MyDrive/med_data/'):\n",
        "    \n",
        "    self.root = root\n",
        "    self.index_map = {}\n",
        "    self.index_map_out = {}\n",
        "\n",
        "    # Create index map for __getitem__\n",
        "\n",
        "    index = 0\n",
        "    for state in os.listdir(self.root):\n",
        "      for path in os.listdir(self.root + state + '/'):\n",
        "\n",
        "        self.index_map[index] = (state,path)\n",
        "        self.index_map_out[index] = (state,path[:3])\n",
        "\n",
        "        index = index + 1\n",
        "    \n",
        "    self.length = index\n",
        "    # For eg, self.index_map[0] = (state,path)\n",
        " \n",
        "  # Define len method\n",
        "  def __len__(self):\n",
        "    \n",
        "    return self.length\n",
        "\n",
        "  # Define get_item method\n",
        "  def __getitem__(self,index):\n",
        "\n",
        "    state,path = self.index_map[index]\n",
        "    combined_path = self.root + state + '/' + path\n",
        "\n",
        "    # Load data into __getitem__\n",
        "    data = np.loadtxt(combined_path).T\n",
        "    \n",
        "    # Epoch and stack data\n",
        "    arr = []\n",
        "\n",
        "    n_seconds = 1\n",
        "    for start in range(0,data.shape[1],128*n_seconds):\n",
        "    \n",
        "      if start + 128*n_seconds < data.shape[1]:\n",
        "        arr.append(data[:,start:start + 128*n_seconds])\n",
        "\n",
        "    # Convert to array, dimensions [n_samples,channels,time]\n",
        "    X = np.array(arr)\n",
        "    Y = [(state,path[:3])]*X.shape[0]\n",
        "\n",
        "    sample = (torch.from_numpy(X),Y)\n",
        "\n",
        "    return sample\n",
        "\n",
        "  # Helper function to return indexes\n",
        "  def return_index(self):\n",
        "\n",
        "    return self.index_map_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr886X8P9zLB"
      },
      "source": [
        "subjects = [subject for state,subject in EEGDataset().return_index().values()]\n",
        "states = [state for state,subject in EEGDataset().return_index().values()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wquIGm-rxFgY"
      },
      "source": [
        "from torch.utils.data.sampler import Sampler\n",
        "import random\n",
        "\n",
        "## Create Custom Train Test Samplers\n",
        "\n",
        "'''\n",
        "\n",
        "  For EEG data, we create a custom train,test sampler which takes indices \n",
        "  corresponding to only one subject as test, and rest as train. The samplers are\n",
        "  then used to train on all subjects barring one.\n",
        "  The samplers retain indices corresponding to the values in index_map\n",
        "\n",
        "'''\n",
        "\n",
        "def create_samplers(dataset,leave_out_subject):\n",
        "\n",
        "  # Create a sampler over training indices, based on index values\n",
        "\n",
        "  class TrainSampler(Sampler):\n",
        "\n",
        "    def __init__(self,dataset,batch_size,leave_out_subject = leave_out_subject):\n",
        "\n",
        "      self.indexes = dataset.return_index()\n",
        "      self.train_indices = [key for key,value in self.indexes.items() if value[1] != leave_out_subject]\n",
        "    \n",
        "    def __iter__(self):\n",
        "      random.shuffle(self.train_indices)\n",
        "      \n",
        "      return iter(self.train_indices)\n",
        "    \n",
        "    def __len__(self):\n",
        "\n",
        "      return len(self.train_indices)\n",
        "  \n",
        "  # Create a sampler over test indices, based on index values\n",
        "\n",
        "  class TestSampler(Sampler):\n",
        "\n",
        "    def __init__(self,dataset,batch_size,leave_out_subject = leave_out_subject):\n",
        "\n",
        "      self.indexes = dataset.return_index()\n",
        "      self.test_indices = [key for key,value in self.indexes.items() if value[1] == leave_out_subject]\n",
        "    \n",
        "    def __iter__(self):\n",
        "      random.shuffle(self.test_indices)\n",
        "      \n",
        "      return iter(self.test_indices)\n",
        "    \n",
        "    def __len__(self):\n",
        "\n",
        "      return len(self.test_indices)\n",
        "  \n",
        "  return TrainSampler(dataset,leave_out_subject),TestSampler(dataset,leave_out_subject)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8r-Ocr2FIBH"
      },
      "source": [
        "# Custom Collate Function, stacks batches by concatenating along batch_dimension directly\n",
        "\n",
        "def custom_collate(batch):\n",
        "\n",
        "  X = [sample[0] for sample in batch]\n",
        "  Y = [sample[1] for sample in batch]\n",
        "  X = np.concatenate(X,axis = 0)\n",
        "  Y = np.concatenate(Y)\n",
        "\n",
        "  return torch.from_numpy(X),Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptmz4EnNdqcx"
      },
      "source": [
        "## First CONV Block - Temporal\n",
        "'''\n",
        "  Takes multivariate time series of the form [Channels,Timestamps] \n",
        "  Applies channelwise convolution to each time series, with n_filters = 10,\n",
        "  for each time series, given output of [Channels, n_filters, Timestamps_new]\n",
        "\n",
        "  Things to check : Number of Filters, Length fo Filter, LeakyReLU\n",
        "'''\n",
        "\n",
        "class Multiple1DTemporalBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, channels = 61, conv_channels = 10, kernel_size = 60):\n",
        "    super().__init__()\n",
        "\n",
        "    self.channels = channels\n",
        "    self.Conv1DLayers = nn.ModuleList()\n",
        "\n",
        "    # Create a Conv1D filter for each time series\n",
        "\n",
        "    for channel in range(self.channels):\n",
        "\n",
        "      self.Conv1DLayers.append(\n",
        "          nn.Conv1d(in_channels = 1, \n",
        "                    out_channels = conv_channels, \n",
        "                    kernel_size = kernel_size,\n",
        "                    stride = 1,\n",
        "                    padding = 1)\n",
        "          )\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    out = []\n",
        "\n",
        "    for channel in range(self.channels):\n",
        "      \n",
        "      x_channel = x[:,[channel],:]\n",
        "      out.append(self.Conv1DLayers[channel](x_channel))\n",
        "    \n",
        "    # Stack on new dimension, output is 4D matrix\n",
        "    out = torch.stack(out, dim = 1)\n",
        "\n",
        "    # Pass through ReLU\n",
        "    out = F.leaky_relu(out,0.01)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh-xi8w6cw1v"
      },
      "source": [
        "## Second CONV Block - Spatial\n",
        "\n",
        "'''\n",
        "  Input from previous CONV block is of the form [Channels,Filters,Timestamps] \n",
        "  We wish to now compute filters to derive spatial features across channels.\n",
        "  This data can then be fed through further convolutions or into an LSTM.\n",
        "\n",
        "  We create a new 'channels' dimension, as required by pytorch and pass in \n",
        "  kernels of the shape (61,10,1) which are passed along the time axis (455).\n",
        "  We pass n_filters = 15 kernels like so, and squeeze the output back to\n",
        "  [Filters, Timestamps]\n",
        "\n",
        "'''\n",
        "\n",
        "class SpatialConvBlock(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # We use a conv1d filter and each convolution\n",
        "    # is on the entire spatial dimension at time t.\n",
        "    \n",
        "    self.conv1 = nn.Conv1d(\n",
        "        in_channels = 1, out_channels = 5, kernel_size = (61,10,1)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    # Perform unsqueeze and squeeze to convert to requisite dimensions\n",
        "\n",
        "    # [None,1,61,10,455]\n",
        "    x = torch.unsqueeze(x, dim = 1)\n",
        "\n",
        "    # [None,15,1,1,455]\n",
        "    x = self.conv1(x)\n",
        "    x = F.leaky_relu(x,0.01)\n",
        "\n",
        "    x = torch.squeeze(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Boff-NLYqYzA"
      },
      "source": [
        "## Linear Layer\n",
        "\n",
        "'''\n",
        "  Create fully connected layers.\n",
        "  Variable number of layers and sizes.\n",
        "  Input dimensions -> [batch, channels, steps]\n",
        "\n",
        "'''\n",
        "class LinearLayers(nn.Module):\n",
        "\n",
        "  def __init__(self,layer_sizes):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_layers = len(layer_sizes)\n",
        "\n",
        "    self.layerlist = nn.ModuleList()\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "\n",
        "      self.layerlist.append(nn.LazyLinear(layer_sizes[i]))\n",
        "    \n",
        "    self.fc = nn.LazyLinear(2)\n",
        "  \n",
        "  def forward(self,x):\n",
        "\n",
        "    x = x.view(x.shape[0],-1)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "\n",
        "      x = self.layerlist[i](x)\n",
        "      x = F.leaky_relu(x, 0.01)\n",
        "\n",
        "    x = self.fc(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH7G8fvay_zw"
      },
      "source": [
        "## Combine all models into a pipeline\n",
        "\n",
        "class EEGModel(nn.Module):\n",
        "\n",
        "  def __init__(self) :\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv_temp = Multiple1DTemporalBlock()\n",
        "    self.conv_spat = SpatialConvBlock()\n",
        "    self.linear_model = LinearLayers([1024,512])\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x = self.conv_temp(x)\n",
        "\n",
        "    x = self.conv_spat(x)\n",
        "\n",
        "    x = self.linear_model(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VITXXuxhJ0Tn"
      },
      "source": [
        "## Helper Function for visualizing gradient flow\n",
        "# Use for debugging inside train loop\n",
        "\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "def plot_grad_flow(named_parameters):\n",
        "\n",
        "    '''\n",
        "      Plots the gradients flowing through different layers in the net during training.\n",
        "      Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "      \n",
        "      Usage: Plug this function in Trainer class after loss.backwards() as \n",
        "      \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow\n",
        "    '''\n",
        "\n",
        "    ave_grads = []\n",
        "    max_grads= []\n",
        "    layers = []\n",
        "\n",
        "    for n, p in named_parameters:\n",
        "\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "\n",
        "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyJftRVBxRw7"
      },
      "source": [
        "def train_model(model,optimizer,subject_id,dataset = EEGDataset(), epochs = 500, batch_size = 32):\n",
        "\n",
        "  '''\n",
        "    Defines the training block.\n",
        "\n",
        "    Inputs: \n",
        "      model for evaluation.\n",
        "      dataset.\n",
        "      epochs.\n",
        "      subject_id for test.\n",
        "      batch_size for sampler.\n",
        "\n",
        "    The model is first trained ONLY using trainloader. All subjects except for\n",
        "    the test subject\n",
        "    \n",
        "\n",
        "  '''\n",
        "\n",
        "  # Load stratified data samplers\n",
        "  train_sampler, test_sampler = create_samplers(dataset,subject_id)\n",
        "\n",
        "  # Load PyTorch dataloader objects  \n",
        "  trainloader = DataLoader(dataset, sampler = train_sampler, \n",
        "                           batch_size = batch_size, collate_fn = custom_collate)\n",
        "  testloader = DataLoader(dataset, sampler = test_sampler, \n",
        "                          batch_size = batch_size, collate_fn = custom_collate)\n",
        "\n",
        "  # Push model to GPU\n",
        "  device = 'cuda'\n",
        "\n",
        "  model = model.to(device = device)\n",
        "  dtype = torch.float32\n",
        "\n",
        "  # Define loss function\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Empty variable to collect loss over time\n",
        "  losses = []\n",
        "\n",
        "  ### Train Block\n",
        "\n",
        "  # Iterate over train loop\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print('Epoch Number = ' + str(epoch))\n",
        "\n",
        "    for t,(x,y) in enumerate(trainloader):\n",
        "      \n",
        "      # model in training mode\n",
        "      model.train()\n",
        "\n",
        "      # Push data,targets to GPU\n",
        "      x = x.to(device = device, dtype = dtype)\n",
        "      x = x.float()\n",
        "\n",
        "      # Convert labels to LongTensor format\n",
        "      y = torch.LongTensor([1 if state == 'med' else 0 for state in y[:,0]])\n",
        "      y = y.to(device = device)\n",
        "\n",
        "      # Compute class scores (Nx2) array\n",
        "      scores = model(x)\n",
        "      \n",
        "      # Visualize confusion matrix, use detach() to extract PyTorch tensor as \n",
        "      # a numpy object\n",
        "      print(pd.crosstab(np.argmax(scores.cpu().detach().numpy(),axis = 1),\n",
        "                        y.cpu().detach().numpy(), rownames = ['pred'], \n",
        "                        colnames = ['true']))\n",
        "      \n",
        "      # Compute loss\n",
        "      # NOTE: Here, we use a weighted loss function to account for class imbalance\n",
        "      loss = nn.CrossEntropyLoss(weight = torch.FloatTensor([0.2 , 0.8]).cuda())(scores,y)\n",
        "\n",
        "      # Append loss of current iter to total losses\n",
        "      losses.append(loss)\n",
        "\n",
        "      # Set all grads to zero\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Compute all backward gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Visualize backwards gradients\n",
        "      plot_grad_flow(model.named_parameters())\n",
        "\n",
        "      # Compute new weights\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print iter loss\n",
        "      print('loss = ',round(loss.item(),2))\n",
        "      print('\\n')\n",
        "\n",
        "    # Visualize complete training loss\n",
        "    plt.plot(losses)\n",
        "    plt.show()\n",
        "\n",
        "  ## Test Loop\n",
        "\n",
        "  # Compute predictions over test data (loaded in batches due to memory constraint)\n",
        "  \n",
        "  accuracy = []\n",
        "\n",
        "  # Iterate over testloader\n",
        "  for t,(x_test,y_test) in enumerate(testloader):\n",
        "    \n",
        "    # Set model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Push data,targets to GPU\n",
        "    x_test = x_test.to(device = device, dtype = dtype)\n",
        "    x_test = x_test.float()\n",
        "\n",
        "    # Convert labels to LongTensor format\n",
        "    y_test = torch.LongTensor([1 if state == 'med' else 0 for state in y_test[:,0]])\n",
        "    y_test = y_test.to(device = device)\n",
        "\n",
        "    # Compute test scores\n",
        "    test_scores = model(x_test)\n",
        "    \n",
        "    # Append accuracy of epoch to accuracy list\n",
        "    accuracy.append(np.mean(np.argmax(scores.cpu().detach().numpy(),axis = 1) == y_test.cpu().detach().numpy()))\n",
        "\n",
        "  print(accuracy,np.mean(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_HGAAH35iOA"
      },
      "source": [
        "## Helper function to run model\n",
        "\n",
        "  '''\n",
        "    Adam optimizer used by default. Specify changes to optimizer within function call\n",
        "    run_model takes in subject id of the form '101','102'...etc. to analyze \n",
        "    subject wise performance. The custom trainloaders take train data as all\n",
        "    subjects other than the subject id and test loaders take all data only\n",
        "    belonging to subject id.\n",
        "  '''\n",
        "\n",
        "def run_model(subject_id, batch_size = 4):\n",
        "\n",
        "  model = EEGModel()\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  batch_size = batch_size\n",
        "\n",
        "  train_model(model,optimizer,subject_id,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ngcJ9kNxZGr2",
        "outputId": "85c97865-ebc8-450b-a3a3-624e9d3177e0"
      },
      "source": [
        "subject_id = '101'\n",
        "model = EEGModel()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "batch_size = 4\n",
        "\n",
        "train_model(model,optimizer,subject_id,batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:175: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Number = 0\n",
            "true   0\n",
            "pred    \n",
            "0     73\n",
            "1     18\n",
            "loss =  0.64\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "0     67  591\n",
            "loss =  10.95\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "0     78  217\n",
            "loss =  6.21\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     80  309\n",
            "loss =  0.65\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     120\n",
            "loss =  1.04\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     80\n",
            "loss =  1.11\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "0      17\n",
            "1     133\n",
            "loss =  0.72\n",
            "\n",
            "\n",
            "true   0     1\n",
            "pred          \n",
            "0     32  1005\n",
            "1      0    84\n",
            "loss =  1.06\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "0     99  243\n",
            "1      3   17\n",
            "loss =  1.06\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "0     133\n",
            "loss =  0.37\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "0     67  663\n",
            "1      0    1\n",
            "loss =  1.23\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "0     94\n",
            "1     13\n",
            "loss =  0.55\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "0     85  218\n",
            "1      6    6\n",
            "loss =  0.9\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "0     36  253\n",
            "1     24  135\n",
            "loss =  0.72\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     64  306\n",
            "loss =  0.66\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     95\n",
            "loss =  0.78\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     63  190\n",
            "loss =  0.64\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     111\n",
            "loss =  0.78\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     33  710\n",
            "loss =  0.62\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     78  209\n",
            "loss =  0.64\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     81  254\n",
            "loss =  0.64\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     105\n",
            "loss =  0.87\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     84  206\n",
            "loss =  0.6\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     74  348\n",
            "loss =  0.59\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     78  315\n",
            "loss =  0.58\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     98  356\n",
            "loss =  0.58\n",
            "\n",
            "\n",
            "true    0    1\n",
            "pred          \n",
            "1     109  379\n",
            "loss =  0.49\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     114\n",
            "loss =  1.01\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     93  247\n",
            "loss =  0.54\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     137\n",
            "loss =  1.13\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     63  344\n",
            "loss =  0.54\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     101\n",
            "loss =  1.0\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     99  366\n",
            "loss =  0.54\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     90  461\n",
            "loss =  0.51\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     79  328\n",
            "loss =  0.58\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     90\n",
            "loss =  1.01\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     90  259\n",
            "loss =  0.57\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     108\n",
            "loss =  0.92\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     84  330\n",
            "loss =  0.58\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     98\n",
            "loss =  0.87\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     66  129\n",
            "loss =  0.61\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     65  866\n",
            "loss =  0.59\n",
            "\n",
            "\n",
            "true    0    1\n",
            "pred          \n",
            "1     109  366\n",
            "loss =  0.61\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     95\n",
            "loss =  0.8\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     45  502\n",
            "loss =  0.58\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     127\n",
            "loss =  0.82\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     72  628\n",
            "loss =  0.6\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     120\n",
            "loss =  0.81\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     72  121\n",
            "loss =  0.6\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     27  794\n",
            "loss =  0.6\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     91  375\n",
            "loss =  0.56\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     82\n",
            "loss =  0.81\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     97  159\n",
            "loss =  0.61\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     94\n",
            "loss =  0.85\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     139\n",
            "loss =  0.83\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     125\n",
            "loss =  0.83\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     59  537\n",
            "loss =  0.62\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     121\n",
            "loss =  0.8\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     91  358\n",
            "loss =  0.62\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     106\n",
            "loss =  0.77\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     90\n",
            "loss =  0.76\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     28  882\n",
            "loss =  0.64\n",
            "\n",
            "\n",
            "true    0    1\n",
            "pred          \n",
            "1     101  139\n",
            "loss =  0.66\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     62  670\n",
            "loss =  0.65\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     131\n",
            "loss =  0.74\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     131\n",
            "loss =  0.74\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     96  321\n",
            "loss =  0.66\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     39  299\n",
            "loss =  0.66\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Number = 1\n",
            "true   0    1\n",
            "pred         \n",
            "1     98  326\n",
            "loss =  0.66\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     50  648\n",
            "loss =  0.66\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     91  310\n",
            "loss =  0.66\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     79  337\n",
            "loss =  0.66\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     149\n",
            "loss =  0.73\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "0      1    0\n",
            "1     73  577\n",
            "loss =  0.66\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     129\n",
            "loss =  0.74\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "0      0    9\n",
            "1     53  531\n",
            "loss =  0.66\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "0      0   24\n",
            "1     46  513\n",
            "loss =  0.66\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "0      0    1\n",
            "1     55  410\n",
            "loss =  0.65\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     48  463\n",
            "loss =  0.65\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     57  754\n",
            "loss =  0.64\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     36  553\n",
            "loss =  0.64\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     94\n",
            "loss =  0.74\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     99\n",
            "loss =  0.75\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     92  318\n",
            "loss =  0.65\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     56  697\n",
            "loss =  0.63\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     88  121\n",
            "loss =  0.64\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     116\n",
            "loss =  0.76\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     104\n",
            "loss =  0.77\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     73  648\n",
            "loss =  0.63\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     83  333\n",
            "loss =  0.64\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     108\n",
            "loss =  0.76\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     98  379\n",
            "loss =  0.59\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     78\n",
            "loss =  0.77\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     59  473\n",
            "loss =  0.63\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     38  911\n",
            "loss =  0.62\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     136\n",
            "loss =  0.78\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     98  299\n",
            "loss =  0.64\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     87\n",
            "loss =  0.77\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     74  527\n",
            "loss =  0.63\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     54  255\n",
            "loss =  0.59\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     84  259\n",
            "loss =  0.62\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     81  309\n",
            "loss =  0.62\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     53  399\n",
            "loss =  0.58\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     88  356\n",
            "loss =  0.62\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     58  723\n",
            "loss =  0.55\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     98\n",
            "loss =  0.91\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     105\n",
            "loss =  0.87\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     77  383\n",
            "loss =  0.59\n",
            "\n",
            "\n",
            "true   0\n",
            "pred    \n",
            "1     99\n",
            "loss =  0.86\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     91  254\n",
            "loss =  0.63\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     120\n",
            "loss =  0.81\n",
            "\n",
            "\n",
            "true    0    1\n",
            "pred          \n",
            "1     110  247\n",
            "loss =  0.63\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     119\n",
            "loss =  0.86\n",
            "\n",
            "\n",
            "true    0\n",
            "pred     \n",
            "1     130\n",
            "loss =  0.88\n",
            "\n",
            "\n",
            "true   0    1\n",
            "pred         \n",
            "1     69  315\n",
            "loss =  0.61\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM9cVXWiUlIz"
      },
      "source": [
        "x = torch.rand"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}